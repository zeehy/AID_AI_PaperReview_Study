# AID PaperReview Study

## Rules
- 매주 수요일 19시 (대면)
- 발표 제외자는 comments 작성 

## Resources
- 📄 **[원문 (PDF)](https://github.com/zeehy/AID_AI_PaperReview_Study/tree/main/pdfs)**  
  논문의 원문은 `pdfs` 디렉토리에서 확인하실 수 있습니다.

- 📊 **[발표 자료](https://github.com/zeehy/AID_AI_PaperReview_Study/tree/main/docs)**  
  발표용 슬라이드 및 관련 자료는 `docs` 디렉토리에 정리되어 있습니다.

- 💬 **[코멘트](https://github.com/zeehy/AID_AI_PaperReview_Study/tree/main/comments)**  
  발표에 대한 피드백 및 리뷰는 `comments` 디렉토리에서 확인 가능합니다.


## Presentations
| 논문명 | 발표자 | 날짜 | 
|--------|--------|--------|
| [Attention Is All You Need](https://github.com/zeehy/AID_AI_PaperReview_Study/blob/main/docs/Attention%20is%20All%20You%20Need_%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C.pdf) | 김태환 | 04/06/25 | 
| [Deep Residual Learning for Image Recognition](https://github.com/zeehy/AID_AI_PaperReview_Study/blob/main/docs/Deep%20Residual%20Learning%20for%20Image%20Recognition.pdf) | 오지현 | 04/06/25 |
| [TBC]() | 강민석 | 04/30/25 | 
| [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://github.com/zeehy/AID_AI_PaperReview_Study/blob/main/docs/VIT.pdf) | 이치오 | 04/30/25 |
| [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://github.com/zeehy/AID_AI_PaperReview_Study/blob/main/docs/Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.pdf) | 김태환 | 05/11/25 | 
| [TBC]() | 오지현 | 05/11/25 |
| [TBC]() | 강민석 | 05/14/25 | 
| [Masked Autoencoders Are Scalable Vision Learners]() | 이치오 | 05/14/25 |


## Members
- [오지현](https://github.com/zeehy)
- [강민석](https://github.com/myeolinmalchi)
- [김태환]()
- [이치오](https://github.com/cho104)
